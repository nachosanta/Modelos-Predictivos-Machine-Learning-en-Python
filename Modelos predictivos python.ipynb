{"cells":[{"cell_type":"markdown","metadata":{"id":"7rcVUrsbBlhT"},"source":["# TP Industrias X.0"],"id":"7rcVUrsbBlhT"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ni7PH09tBlhY"},"outputs":[],"source":["# librerias\n","#!pip install numpy\n","#!pip install pandas\n","#!pip install matplotlib.pyplot\n","#!pip install seaborn\n","import numpy as np\n","import pandas as pd\n","import math as mh\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import sklearn as sk"],"id":"Ni7PH09tBlhY"},{"cell_type":"code","execution_count":null,"metadata":{"id":"B951K75xBlhb"},"outputs":[],"source":["df_train = pd.read_csv('Bank Marketing train_set.csv')"],"id":"B951K75xBlhb"},{"cell_type":"code","execution_count":null,"metadata":{"id":"AB0lLYTFBlhc"},"outputs":[],"source":["df_test = pd.read_csv('Bank Marketing test_set.csv')"],"id":"AB0lLYTFBlhc"},{"cell_type":"markdown","metadata":{"id":"K6Macua3Blhc"},"source":["## ***Analisis Exploratorio de datos***"],"id":"K6Macua3Blhc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"awjsyj6FBlhd"},"outputs":[],"source":["# que columnas tienen los datos?\n","print(df_train.columns)\n","cols = df_train.columns"],"id":"awjsyj6FBlhd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"kA1zZ3eNBlhe"},"outputs":[],"source":["df_train.head() #observo las primeras 5 filas del df"],"id":"kA1zZ3eNBlhe"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yoh9rJEyBlhf"},"outputs":[],"source":["# qué tamaño tienen los datos?\n","print(df_train.shape)"],"id":"Yoh9rJEyBlhf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"tr-Tsn5UBlhg"},"outputs":[],"source":["# hay valores nulos en los datos?\n","print(df_train.info())\n","#object tipo string. Int, float son tipo numerico\n","#valores non null menores a 38429 indican valores nulos en ese campo"],"id":"tr-Tsn5UBlhg"},{"cell_type":"code","execution_count":null,"metadata":{"id":"JC1EqbBMBlhh"},"outputs":[],"source":["#Valores nulos totales\n","df_train.isna().sum().sort_values().plot(kind='bar')"],"id":"JC1EqbBMBlhh"},{"cell_type":"markdown","metadata":{"id":"CFstjhBsBlhi"},"source":["Se observan valores **nulos** en las variables: ***age, job, Balance y Pdays***. Principalmente en Age y Balance.\n","\n","Más adelante, habrá que tratar estos casos de algún modo."],"id":"CFstjhBsBlhi"},{"cell_type":"code","execution_count":null,"metadata":{"id":"qucwTHE_Blhi"},"outputs":[],"source":["# como se distribuyen las variables numéricas\n","print(df_train.describe())"],"id":"qucwTHE_Blhi"},{"cell_type":"markdown","metadata":{"id":"LmZ1PExbBlhj"},"source":["Ademas, podemos hacer una comparación entre media y mediana para analizar si estas distribuciones son simétricas o no (si la media y la mediana son cercanas, la dist es más simétrica).\n","\n","A mayor simetría de la distribución, más podría aproximarce la variable a una Normal, lo cual es beneficioso en los modelos de Machine Learning más probabilisticos como la Regresión Logística."],"id":"LmZ1PExbBlhj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"jFbDDs0MBlhk"},"outputs":[],"source":["df_train.median().sort_values().plot(kind='barh')"],"id":"jFbDDs0MBlhk"},{"cell_type":"code","execution_count":null,"metadata":{"id":"DR9dJ1qDBlhk"},"outputs":[],"source":["# diferencia porcentual entre media y mediana para las variables:\n","((df_train.mean()-df_train.median())/df_train.mean())*100"],"id":"DR9dJ1qDBlhk"},{"cell_type":"markdown","metadata":{"id":"R1rCVASJBlhl"},"source":["-------------------------------------------------------\n","Obsevamos que la mayoría de las variables **no** poseen medias y medianas cercanas, por lo que estarían lejos de ser simétricas.\n","\n","Se puede continuar este análsis de manera gráfica, armando los histogramas de cada variable:"],"id":"R1rCVASJBlhl"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ahZRqHRbBlhl"},"outputs":[],"source":["# histogramas\n","\n","for i in cols:\n","    if (df_train[i].dtype != 'O')&(i!='Last Contact Day'):\n","        print('Variable:',i)\n","        plt.figure(figsize=(10,5))\n","        sns.histplot(df_train[i])\n","        plt.show()\n"],"id":"ahZRqHRbBlhl"},{"cell_type":"markdown","metadata":{"id":"uevvSyMtBlhm"},"source":["En los histogramas de varias variables se observa mucha asimetría positiva (similar a dist log-Normal), por lo que si más adelante necesitáramos que estas distribuaciones se asemejen a una Normal, podríamos aplicar una transformación (ej logarítmica) para volverlas más asimétricas."],"id":"uevvSyMtBlhm"},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHtgrHZWBlhm"},"outputs":[],"source":["#sns.pairplot(df_train[['Age','Balance (euros)','Housing Loan','Last Contact Day','Last Contact Duration','Campaign','Pdays','Previous','Subscription']], hue = 'Subscription')"],"id":"uHtgrHZWBlhm"},{"cell_type":"code","execution_count":null,"metadata":{"id":"35PH2nukBlhn"},"outputs":[],"source":["#estudio de correlacion de variables con valores nulos vs Y (suscrpcion)\n","#df_train_nan = df_train[['Age','Job','Balance (euros)','Pdays','Subscription']] #variables con NAs en train\n","sns.heatmap(df_train[['Age','Balance (euros)','Housing Loan','Last Contact Day','Last Contact Duration','Campaign','Pdays','Previous','Subscription']].corr(), annot=True, cmap=\"YlGnBu\")\n","plt.show()"],"id":"35PH2nukBlhn"},{"cell_type":"markdown","metadata":{"id":"Cuv7dpJ_Blhn"},"source":["No Hay correlaciones fuerte 1o1 entre las variables, siendo significativamente superior la correlacion entre suscripcion y last call duration, pdays y previous y en menor medida campaign con last contact day."],"id":"Cuv7dpJ_Blhn"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3VKIPBCoBlhn"},"outputs":[],"source":["# nro de valores únicos por variable\n","df_train.nunique().sort_values()"],"id":"3VKIPBCoBlhn"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7u0KbhFBlho"},"outputs":[],"source":["# como se comportan las variables categóricas\n","df_train.describe(include=['O'])"],"id":"f7u0KbhFBlho"},{"cell_type":"markdown","metadata":{"id":"5LuK7AXjBlho"},"source":["Vemos que el trabajo mas frecuente es blue-collar, hay mayor cantidad de casados que solteros, la mayoria tuvo al menos educacion secundaria y pidieron prestamos hipotecarios. La myoria no defaulteo ni tiene prestamos personales. El contacto de preferencia es por celular. El mes de mayor cantidad de contactos fue Mayo."],"id":"5LuK7AXjBlho"},{"cell_type":"code","execution_count":null,"metadata":{"id":"uccDeKlIBlho"},"outputs":[],"source":["# grafico de barras con las etiquetas de las cantidades\n","fig_ms, ax_ms = plt.subplots()\n","y_ms = df_train['Marital Status'].value_counts()\n","\n","hbars_ms = ax_ms.barh(y_ms.keys(), y_ms, align='center')\n","ax_ms.invert_yaxis()  # para que aparezca primero las cat de mayor frec\n","ax_ms.set_xlim(right=max(y_ms)+0.25*max(y_ms))\n","ax_ms.set_title('¿Cuánta gente está casada, soltera y divorciada?')\n","\n","#ax_ms.bar_label(hbars_ms) # así podría agregar labels si tuviera la ultima version de matplotlib\n","for i, v in enumerate(y_ms):\n","    plt.text(v, i, \"  \" + str(v), color='black', va='center', fontweight='bold')\n","\n","plt.show()"],"id":"uccDeKlIBlho"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oc33gozvBlhp"},"outputs":[],"source":["# grafico de barras con las etiquetas de las cantidades\n","fig_ed, ax_ed = plt.subplots()\n","y_ed = df_train['Education'].value_counts()\n","\n","hbars_ed = ax_ed.barh(y_ed.keys(), y_ed, align='center')\n","ax_ed.invert_yaxis()  # para que aparezca primero las cat de mayor frec\n","ax_ed.set_xlim(right=max(y_ed)+0.25*max(y_ed))\n","ax_ed.set_title('¿Cuánta gente tiene educación primaria, secundaria y terciaria?')\n","\n","#ax_ed.bar_label(hbars_ed) # así podría agregar labels si tuviera la ultima version de matplotlib\n","for i, v in enumerate(y_ed):\n","    plt.text(v, i, \"  \" + str(v), color='black', va='center', fontweight='bold')\n","\n","plt.show()"],"id":"Oc33gozvBlhp"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yw6jFXGDBlhp"},"outputs":[],"source":["#cambio valores 1 y 2 a binario 0, 1 para decir si estan suscritos o no\n","df_train['Subscription']=df_train['Subscription'].replace(1,0)\n","df_train['Subscription']=df_train['Subscription'].replace(2,1)\n","df_train.groupby(['Subscription']).count()['Education']"],"id":"Yw6jFXGDBlhp"},{"cell_type":"code","execution_count":null,"metadata":{"id":"205Q2QkyBlhp"},"outputs":[],"source":["# grafico de barras con las etiquetas de las cantidades\n","fig_sub, ax_sub = plt.subplots()\n","y_sub = df_train['Subscription'].value_counts()\n","\n","hbars_sub = ax_sub.barh(y_sub.keys().astype(str), y_sub, align='center')\n","ax_sub.invert_yaxis()  # para que aparezca primero las cat de mayor frec\n","ax_sub.set_xlim(right=max(y_sub)+0.25*max(y_sub))\n","ax_sub.set_title('¿Cuánta gente se suscribió al plazo fijo y cuantas no?')\n","\n","#ax_sub.bar_label(hbars_sub) # así podría agregar labels si tuviera la ultima version de matplotlib\n","for i, v in enumerate(y_sub):\n","    plt.text(v, i, \"  \" + str(v), color='black', va='center', fontweight='bold')\n","\n","plt.show()"],"id":"205Q2QkyBlhp"},{"cell_type":"markdown","metadata":{"id":"cxqGU5T9Blhq"},"source":["## ***Limpieza de los datos***\n"],"id":"cxqGU5T9Blhq"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xlx6fyWQBlhq"},"outputs":[],"source":["df_train.isna().sum().sort_values() #train"],"id":"Xlx6fyWQBlhq"},{"cell_type":"code","execution_count":null,"metadata":{"id":"GAuieFvqBlhq"},"outputs":[],"source":["df_test.isna().sum() #test"],"id":"GAuieFvqBlhq"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYwJ97UbBlhq"},"outputs":[],"source":["df_train.groupby(['Subscription','Job']).count()['Education']"],"id":"ZYwJ97UbBlhq"},{"cell_type":"markdown","metadata":{"id":"4FOfRx3oBlhr"},"source":["Unknown representa un % muy bajo para ambos grupos, suscritos y no suscritos"],"id":"4FOfRx3oBlhr"},{"cell_type":"markdown","metadata":{"id":"W6Uf9GLDBlhr"},"source":["Vamos a generar 3 dataframes, \n","1) eliminando datos faltantes ya que hay baja correlacion con la variable a predecir.\n","\n","2) reemplazando los valores faltantes por mediana en caso de series numericas y \"Desconocido\" para categoricas.\n","\n","3) reemplazando los valores faltantes numericos por el promedio y \"Desconocido\" para categoricas."],"id":"W6Uf9GLDBlhr"},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZdvsbSaBlhr"},"outputs":[],"source":["#caso numero 1 \n","df_train_1 = df_train.copy()\n","df_train_1 = df_train_1.dropna()"],"id":"hZdvsbSaBlhr"},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDTz4Qi2Blhr"},"outputs":[],"source":["df_train_1.isna().sum()"],"id":"nDTz4Qi2Blhr"},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLgTlOoHBlhs"},"outputs":[],"source":["#caso numero 2 \n","l1 =[]\n","for i in ['Age','Balance (euros)','Pdays']:\n","    m = df_train[i].median()\n","    l1.append(m)\n","print(l1)"],"id":"xLgTlOoHBlhs"},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNNiVg-zBlhs"},"outputs":[],"source":["df_train_2 = df_train.copy()\n","df_train_2['Age']=df_train_2['Age'].fillna(l1[0])\n","df_train_2['Balance (euros)']=df_train_2['Balance (euros)'].fillna(l1[1])\n","df_train_2['Pdays']=df_train_2['Pdays'].fillna(l1[2])\n","df_train_2['Job']=df_train_2['Job'].fillna('unknown')"],"id":"PNNiVg-zBlhs"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRd7zuS8Blht"},"outputs":[],"source":["df_train_2.isna().sum()"],"id":"bRd7zuS8Blht"},{"cell_type":"code","execution_count":null,"metadata":{"id":"84waFE-ZBlht"},"outputs":[],"source":["#caso numero 3 \n","\n","l2 =[]\n","for i in ['Age','Balance (euros)','Pdays']:\n","    m = df_train[i].mean()\n","    l2.append(round(m,2))\n","print(l2)"],"id":"84waFE-ZBlht"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dRdamZBwBlhu"},"outputs":[],"source":["df_train_3 = df_train.copy()\n","df_train_3['Age']=df_train_2['Age'].fillna(l2[0])\n","df_train_3['Balance (euros)']=df_train_2['Balance (euros)'].fillna(l2[1])\n","df_train_3['Pdays']=df_train_2['Pdays'].fillna(l2[2])\n","df_train_3['Job']=df_train_2['Job'].fillna('unknown')"],"id":"dRdamZBwBlhu"},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFfOgVpzBlhu"},"outputs":[],"source":["df_train_3.isna().sum()"],"id":"sFfOgVpzBlhu"},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHV9L1KhBlhu"},"outputs":[],"source":["#nuevas variables \n","\n","# podemos agrupar las dos variables de prestamos para saber si tiene 1 2 o ninguno \n","    # 1+1 = 2\n","    # 1+0 = 1\n","    # 0+1 = 1\n","    # 0+0 = 0\n","# tenemos deudor nivel 0, 1 y 2\n","#df_train_p=df_train.copy()\n","\n","# set de training\n","df_train_1['Loan']=np.where(df_train_1['Loan']=='yes',1,0)\n","df_train_1['Housing Loan']=np.where(df_train_1['Housing Loan']=='yes',1,0)\n","df_train_1['Total_loan']=df_train_1['Loan']+df_train_1['Housing Loan']\n","\n","df_train_2['Loan']=np.where(df_train_2['Loan']=='yes',1,0)\n","df_train_2['Housing Loan']=np.where(df_train_2['Housing Loan']=='yes',1,0)\n","df_train_2['Total_loan']=df_train_2['Loan']+df_train_2['Housing Loan']\n","\n","df_train_3['Loan']=np.where(df_train_3['Loan']=='yes',1,0)\n","df_train_3['Housing Loan']=np.where(df_train_3['Housing Loan']=='yes',1,0)\n","df_train_3['Total_loan']=df_train_3['Loan']+df_train_3['Housing Loan']\n","\n","# set de testing\n","df_test['Loan']=np.where(df_test['Loan']=='yes',1,0)\n","df_test['Housing Loan']=np.where(df_test['Housing Loan']=='yes',1,0)\n","df_test['Total_loan']=df_test['Loan']+df_test['Housing Loan']\n"],"id":"iHV9L1KhBlhu"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"oe_Ln4CyBlhu","executionInfo":{"status":"error","timestamp":1669669441640,"user_tz":180,"elapsed":87,"user":{"displayName":"Facundo Cagnasso","userId":"13825862613160771732"}},"outputId":"9a8cb57d-9870-4bed-aa29-c3766ba706f9"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-06fb0c9edba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmap_dictionary\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'nivel 0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m'nivel 1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"nivel 2\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_train_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Deudor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Total_loan'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf_train_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Deudor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Total_loan'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf_train_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Deudor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Total_loan'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_train_1' is not defined"]}],"source":["def set_values(row, value):\n","    return value[row]\n","   \n","map_dictionary ={0 : 'nivel 0', 1 :'nivel 1', 2 : \"nivel 2\"} \n","   \n","df_train_1['Deudor'] = df_train_1['Total_loan'].apply(set_values, args =(map_dictionary, ))\n","df_train_2['Deudor'] = df_train_2['Total_loan'].apply(set_values, args =(map_dictionary, ))\n","df_train_3['Deudor'] = df_train_3['Total_loan'].apply(set_values, args =(map_dictionary, ))\n","df_test['Deudor'] = df_test['Total_loan'].apply(set_values, args =(map_dictionary, ))\n","df_train_1.drop('Total_loan',axis=1,inplace=True)\n","df_train_2.drop('Total_loan',axis=1,inplace=True)\n","df_train_3.drop('Total_loan',axis=1,inplace=True)\n"],"id":"oe_Ln4CyBlhu"},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wkvb47aBlhv"},"outputs":[],"source":["df_train_1['Deudor'].head()"],"id":"-wkvb47aBlhv"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bx1i42foBlhv"},"outputs":[],"source":["df_train_2['Deudor'].head()"],"id":"Bx1i42foBlhv"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTNqDyVRBlhv"},"outputs":[],"source":["df_train_3['Deudor'].head()"],"id":"bTNqDyVRBlhv"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ox1QQECNBlhv"},"outputs":[],"source":["df_test['Deudor'].head()"],"id":"ox1QQECNBlhv"},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5PTJnW_Blhw"},"outputs":[],"source":["df_train_1.groupby(['Subscription','Deudor']).count()['Education'].sort_index()"],"id":"U5PTJnW_Blhw"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tGFRWfIBlhw"},"outputs":[],"source":["df_train_1['Deudor'].value_counts().sort_index()"],"id":"4tGFRWfIBlhw"},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7uXwKcGBlhw"},"outputs":[],"source":["df_train_2['Deudor'].value_counts().sort_index()"],"id":"N7uXwKcGBlhw"},{"cell_type":"code","execution_count":null,"metadata":{"id":"QbGUJbqnBlhx"},"outputs":[],"source":["df_train_3['Deudor'].value_counts().sort_index()"],"id":"QbGUJbqnBlhx"},{"cell_type":"code","execution_count":null,"metadata":{"id":"A0eBXI6gBlhx"},"outputs":[],"source":["# Exportar a CSV\n","df_train_1.to_csv('datos_nulos_eliminados.csv',index=False)\n","df_train_2.to_csv('datos_nulos_reemplazados_mediana.csv',index=False)\n","df_train_3.to_csv('datos_nulos_reemplazados_promedio.csv',index=False)"],"id":"A0eBXI6gBlhx"},{"cell_type":"markdown","metadata":{"id":"s7ejwysLBlhx"},"source":["Dado que observamos mucha asimetría en las variables, lo más representativo sería utilizar la mediana para reemplazar los NA."],"id":"s7ejwysLBlhx"},{"cell_type":"markdown","metadata":{"id":"rCzxj-1hBlhx"},"source":["## ***Modelos de Machine Learning***"],"id":"rCzxj-1hBlhx"},{"cell_type":"markdown","metadata":{"id":"GzYUgLgHBlhy"},"source":["#### Se realizan 3 modelos de aprendizaje supervisado para cada uno de los datasets generados previamente"],"id":"GzYUgLgHBlhy"},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1K4OE_4Blhy"},"outputs":[],"source":["# uso el set de datos en el que se reemplazo los nulos por la mediana\n","lista_archivos = [df_train_1,df_train_2,df_train_3]\n"],"id":"G1K4OE_4Blhy"},{"cell_type":"markdown","metadata":{"id":"b-ItyhElBlhy"},"source":["### Preprocesamiento para Modelos"],"id":"b-ItyhElBlhy"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ts1Z9cOGBlhy"},"outputs":[],"source":["#dummies priemr dataframe\n","x_train_ml_1 = df_train_1.loc[:, df_train_1.columns != 'Subscription'] # variables explicativas\n","y_train_ml_1 = df_train_1[['Subscription']] # variables de respuesta\n","x_train_ml_1.head()\n","print(x_train_ml_1[['Job']].value_counts(), '\\n \\n',\n","x_train_ml_1[['Marital Status']].value_counts(), '\\n \\n',\n","x_train_ml_1[['Education']].value_counts(), '\\n \\n',\n","x_train_ml_1[['Default']].value_counts(), '\\n \\n',\n","x_train_ml_1[['Contact']].value_counts(), '\\n \\n',\n","x_train_ml_1[['Poutcome']].value_counts(), '\\n \\n',\n","x_train_ml_1[['Deudor']].value_counts())\n","# Codifico dummies tomando referencia en la categoría más común\n","x_train_ml_dum_1 = pd.get_dummies(x_train_ml_1).drop(['Job_blue-collar',\n","                                              'Marital Status_married',\n","                                              'Education_secondary',\n","                                              'Default_no',\n","                                              'Contact_cellular',\n","                                              'Poutcome_unknown',\n","                                              'Deudor_nivel 1'],\n","                                             axis=1)\n","# hago lo mismo para el set de testing\n","df_test_dum_1 = pd.get_dummies(df_test).drop(['Job_blue-collar',\n","                                            'Marital Status_married',\n","                                            'Education_secondary',\n","                                            'Default_no',\n","                                            'Contact_cellular',\n","                                            'Poutcome_unknown',\n","                                            'Deudor_nivel 1'],\n","                                             axis=1)\n","# analizo las categorias y las cantidades de datos para cada categoría de las variables categóricas\n","# para que me ayude a definir categorias de referencia/base para codificar dummies\n","print(x_train_ml_1[['Job']].value_counts(), '\\n \\n',\n","x_train_ml_1[['Marital Status']].value_counts(), '\\n \\n',\n","x_train_ml_1[['Education']].value_counts(), '\\n \\n',\n","x_train_ml_1[['Default']].value_counts(), '\\n \\n',\n","x_train_ml_1[['Contact']].value_counts(), '\\n \\n',\n","x_train_ml_1[['Poutcome']].value_counts(), '\\n \\n',\n","x_train_ml_1[['Deudor']].value_counts())\n","# Codifico dummies tomando referencia en la categoría más común\n","x_train_ml_dum_1 = pd.get_dummies(x_train_ml_1).drop(['Job_blue-collar',\n","                                                  'Marital Status_married',\n","                                                  'Education_secondary',\n","                                                  'Default_no',\n","                                                  'Contact_cellular',\n","                                                  'Poutcome_unknown',\n","                                                  'Deudor_nivel 1'],\n","                                                   axis=1)\n","# hago lo mismo para el set de testing\n","df_test_dum_1 = pd.get_dummies(df_test).drop(['Job_blue-collar',\n","                                                'Marital Status_married',\n","                                                'Education_secondary',\n","                                                'Default_no',\n","                                                'Contact_cellular',\n","                                                'Poutcome_unknown',\n","                                                'Deudor_nivel 1'],\n","                                                 axis=1)\n","#print(x_train_ml_dum_1.head(5)) \n","#print(y_train_ml_1.head(5))"],"id":"Ts1Z9cOGBlhy"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1oygOLzSBlhz"},"outputs":[],"source":["#dummies segundo dataframe\n","x_train_ml_2 = df_train_2.loc[:, df_train_2.columns != 'Subscription'] # variables explicativas\n","y_train_ml_2 = df_train_2[['Subscription']] # variables de respuesta\n","'''x_train_ml_2.head()\n","print(x_train_ml_2[['Job']].value_counts(), '\\n \\n',\n","x_train_ml_2[['Marital Status']].value_counts(), '\\n \\n',\n","x_train_ml_2[['Education']].value_counts(), '\\n \\n',\n","x_train_ml_2[['Default']].value_counts(), '\\n \\n',\n","x_train_ml_2[['Contact']].value_counts(), '\\n \\n',\n","x_train_ml_2[['Poutcome']].value_counts(), '\\n \\n',\n","x_train_ml_2[['Deudor']].value_counts())'''\n","# Codifico dummies tomando referencia en la categoría más común\n","x_train_ml_dum_2 = pd.get_dummies(x_train_ml_2).drop(['Job_blue-collar',\n","                                              'Marital Status_married',\n","                                              'Education_secondary',\n","                                              'Default_no',\n","                                              'Contact_cellular',\n","                                              'Poutcome_unknown',\n","                                              'Deudor_nivel 1'],\n","                                             axis=1)\n","# hago lo mismo para el set de testing\n","df_test_dum_2 = pd.get_dummies(df_test).drop(['Job_blue-collar',\n","                                            'Marital Status_married',\n","                                            'Education_secondary',\n","                                            'Default_no',\n","                                            'Contact_cellular',\n","                                            'Poutcome_unknown',\n","                                            'Deudor_nivel 1'],\n","                                             axis=1)\n","# analizo las categorias y las cantidades de datos para cada categoría de las variables categóricas\n","# para que me ayude a definir categorias de referencia/base para codificar dummies\n","'''print(x_train_ml_2[['Job']].value_counts(), '\\n \\n',\n","x_train_ml_2[['Marital Status']].value_counts(), '\\n \\n',\n","x_train_ml_2[['Education']].value_counts(), '\\n \\n',\n","x_train_ml_2[['Default']].value_counts(), '\\n \\n',\n","x_train_ml_2[['Contact']].value_counts(), '\\n \\n',\n","x_train_ml_2[['Poutcome']].value_counts(), '\\n \\n',\n","x_train_ml_2[['Deudor']].value_counts())'''\n","# Codifico dummies tomando referencia en la categoría más común\n","x_train_ml_dum_2 = pd.get_dummies(x_train_ml_2).drop(['Job_blue-collar',\n","                                                  'Marital Status_married',\n","                                                  'Education_secondary',\n","                                                  'Default_no',\n","                                                  'Contact_cellular',\n","                                                  'Poutcome_unknown',\n","                                                  'Deudor_nivel 1'],\n","                                                   axis=1)\n","# hago lo mismo para el set de testing\n","df_test_dum_2 = pd.get_dummies(df_test).drop(['Job_blue-collar',\n","                                                'Marital Status_married',\n","                                                'Education_secondary',\n","                                                'Default_no',\n","                                                'Contact_cellular',\n","                                                'Poutcome_unknown',\n","                                                'Deudor_nivel 1'],\n","                                                 axis=1)\n","#x_train_ml_dum_2.head(10) "],"id":"1oygOLzSBlhz"},{"cell_type":"code","execution_count":null,"metadata":{"id":"zvijQBeRBlh0"},"outputs":[],"source":["#dummies segundo dataframe\n","x_train_ml_3 = df_train_3.loc[:, df_train_3.columns != 'Subscription'] # variables explicativas\n","y_train_ml_3 = df_train_3[['Subscription']] # variables de respuesta\n","'''x_train_ml_3.head()\n","print(x_train_ml_3[['Job']].value_counts(), '\\n \\n',\n","x_train_ml_3[['Marital Status']].value_counts(), '\\n \\n',\n","x_train_ml_3[['Education']].value_counts(), '\\n \\n',\n","x_train_ml_3[['Default']].value_counts(), '\\n \\n',\n","x_train_ml_3[['Contact']].value_counts(), '\\n \\n',\n","x_train_ml_3[['Poutcome']].value_counts(), '\\n \\n',\n","x_train_ml_3[['Deudor']].value_counts())'''\n","# Codifico dummies tomando referencia en la categoría más común\n","x_train_ml_dum_3 = pd.get_dummies(x_train_ml_3).drop(['Job_blue-collar',\n","                                              'Marital Status_married',\n","                                              'Education_secondary',\n","                                              'Default_no',\n","                                              'Contact_cellular',\n","                                              'Poutcome_unknown',\n","                                              'Deudor_nivel 1'],\n","                                             axis=1)\n","# hago lo mismo para el set de testing\n","df_test_dum_3 = pd.get_dummies(df_test).drop(['Job_blue-collar',\n","                                            'Marital Status_married',\n","                                            'Education_secondary',\n","                                            'Default_no',\n","                                            'Contact_cellular',\n","                                            'Poutcome_unknown',\n","                                            'Deudor_nivel 1'],\n","                                             axis=1)\n","# analizo las categorias y las cantidades de datos para cada categoría de las variables categóricas\n","# para que me ayude a definir categorias de referencia/base para codificar dummies\n","'''print(x_train_ml_3[['Job']].value_counts(), '\\n \\n',\n","x_train_ml_3[['Marital Status']].value_counts(), '\\n \\n',\n","x_train_ml_3[['Education']].value_counts(), '\\n \\n',\n","x_train_ml_3[['Default']].value_counts(), '\\n \\n',\n","x_train_ml_3[['Contact']].value_counts(), '\\n \\n',\n","x_train_ml_3[['Poutcome']].value_counts(), '\\n \\n',\n","x_train_ml_3[['Deudor']].value_counts())'''\n","# Codifico dummies tomando referencia en la categoría más común\n","x_train_ml_dum_3 = pd.get_dummies(x_train_ml_3).drop(['Job_blue-collar',\n","                                                  'Marital Status_married',\n","                                                  'Education_secondary',\n","                                                  'Default_no',\n","                                                  'Contact_cellular',\n","                                                  'Poutcome_unknown',\n","                                                  'Deudor_nivel 1'],\n","                                                   axis=1)\n","# hago lo mismo para el set de testing\n","df_test_dum_3 = pd.get_dummies(df_test).drop(['Job_blue-collar',\n","                                                'Marital Status_married',\n","                                                'Education_secondary',\n","                                                'Default_no',\n","                                                'Contact_cellular',\n","                                                'Poutcome_unknown',\n","                                                'Deudor_nivel 1'],\n","                                                 axis=1)\n","#x_train_ml_dum_3.head(10) "],"id":"zvijQBeRBlh0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"AM63Ng5TBlh1"},"outputs":[],"source":["x_dummie=[x_train_ml_dum_1,x_train_ml_dum_2,x_train_ml_dum_3]\n","y_dummie=[y_train_ml_1,y_train_ml_2,y_train_ml_3]\n"],"id":"AM63Ng5TBlh1"},{"cell_type":"markdown","metadata":{"id":"vFRN3jTyBlh2"},"source":["A falta de conocimiento sobre un mejor criterio, vamos a tomar la categoría con más datos de cada variable como **referencia para la codificación de dummies**, ya que se supone como el caso más común para tomar de base."],"id":"vFRN3jTyBlh2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0XjBAmmEBlh2"},"outputs":[],"source":["# modulos necesarios para preprocesamiento\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline"],"id":"0XjBAmmEBlh2"},{"cell_type":"markdown","metadata":{"id":"rGx8Sp1oBlh2"},"source":["### Modelo 1: ***Regresion logistica***"],"id":"rGx8Sp1oBlh2"},{"cell_type":"markdown","metadata":{"id":"WmMH7rl2Blh3"},"source":[" Dado la **gran diferencia en ordenes de magnitud de las distintas variables numéricas/continuas** de los datasets. Lo mejor será **estandarizar/escalar** estas variables para entrenar y predecir con los modelos de machine learning."],"id":"WmMH7rl2Blh3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMA_yth3Blh3"},"outputs":[],"source":["variables_num = x_train_ml_dum_2[['Age', 'Balance (euros)', 'Last Contact Day', 'Last Contact Duration', 'Campaign', 'Pdays']].columns.values\n","print(variables_num)"],"id":"sMA_yth3Blh3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"oytrHUVEBlh3"},"outputs":[],"source":["# estandarizo/escalo las variables continuas\n","scaler = StandardScaler()\n","x_train_ml_Num = x_train_ml_dum_2[variables_num]\n","df_test_Num = df_test_dum_2[variables_num]\n","\n","x_train_ml_NumScaled = scaler.fit_transform(x_train_ml_Num)\n","df_test_NumScaled = scaler.transform(df_test_Num)\n","\n","# corroboro que estan estandarizados (aprox media 0 y desvio 1):\n","print(\"Train: media \"+ str(x_train_ml_NumScaled.mean())+ \" y desvio \"+ str(x_train_ml_NumScaled.std()) +\"\\n\"+\n","     \"Test: media \" + str(df_test_NumScaled.mean()) +\" y desvio \"+ str(df_test_NumScaled.std()) )"],"id":"oytrHUVEBlh3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ozWLuX8Blh4"},"outputs":[],"source":["# convierto en dataframe porque es más facil para trabajar \n","x_train_ml_NumScaled = pd.DataFrame(x_train_ml_NumScaled, columns = list(x_train_ml_dum_2[variables_num].columns))\n","df_test_NumScaled = pd.DataFrame(df_test_NumScaled, columns = list(df_test_dum_2[variables_num].columns))"],"id":"3ozWLuX8Blh4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"IDDpF9zNBlh4"},"outputs":[],"source":["x_train_ml_NumScaled.head()"],"id":"IDDpF9zNBlh4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"MGdmQoWgBlh5"},"outputs":[],"source":["df_test_NumScaled.head()"],"id":"MGdmQoWgBlh5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9PX0CZ7Blh5"},"outputs":[],"source":["# junto el df de variables numéricas estandarizadas/escaladas con el df de dummies\n","x_train_ml_dum_scaled = pd.concat([x_train_ml_NumScaled, x_train_ml_dum_2.drop(variables_num, axis=1)], axis=1)\n","df_test_dum_scaled = pd.concat([df_test_NumScaled, df_test_dum_2.drop(variables_num, axis=1)], axis=1)"],"id":"U9PX0CZ7Blh5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"K_peEUkyBlh5"},"outputs":[],"source":["# data set de training con dummies codificadas y variables numéricas estandarizadas\n","x_train_ml_dum_scaled.head(10)"],"id":"K_peEUkyBlh5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjqzgNXHBlh5"},"outputs":[],"source":["# data set de testing con dummies codificadas y variables numéricas estandarizadas\n","del df_test_dum_scaled['Total_loan'] # no lo necesito por la correlacion con Deudor\n","df_test_dum_scaled.head(10)"],"id":"vjqzgNXHBlh5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"me1ai8zvBlh5"},"outputs":[],"source":["# otros modulos necesarios para preprocesamiento\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import RepeatedStratifiedKFold #útil para casos como este donde la variable de respuesta está desbalanceada\n","from sklearn.model_selection import GridSearchCV"],"id":"me1ai8zvBlh5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"EtZ2pMkFBlh5"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression"],"id":"EtZ2pMkFBlh5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"QlEsDDa_Blh6"},"outputs":[],"source":["# NO uso las transformaciones anteriores porque me está tirando NAs, no entiendo bien por que"],"id":"QlEsDDa_Blh6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"xtwVohjZBlh6"},"outputs":[],"source":["# valores de los hiperparámetros posibles para la regresión logística\n","# (tarda unos 10/15 minutos en correr todos los modelos con los distintos parametros)\n","model = LogisticRegression()\n","solvers = ['newton-cg', 'lbfgs', 'liblinear'] # algoritmo de convergencia\n","penalty = ['l2'] # tipo de penalidad (regularización)\n","c_values = [10000000, 1000000, 100000, 10000, 1000] # valores del hiperparámetro para penalidad\n","\n","grid = dict(solver=solvers,penalty=penalty,C=c_values) # armo la grid search\n","\n","# armo los folds de la cross validation de manera estratificada porque son datos desbalanceados\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n","\n","# defino la grid search con los parametros a probar y la cross validation\n","grid_search_logreg = GridSearchCV(estimator=model,\n","                           param_grid=grid,\n","                           n_jobs=-1, # (solo tengo un procesador en mi compu pero aca le pondría que lo corra en todos)\n","                           cv=cv,\n","                           scoring='f1', # como esta desbalanceado uso f1 = 2* (precision*recall)/(precision+recall)\n","                           error_score=0)"],"id":"xtwVohjZBlh6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDLDtoo2Blh-"},"outputs":[],"source":["# entreno el modelo\n","grid_result_logreg = grid_search_logreg.fit(x_train_ml_dum_scaled.values, y_train_ml_2.values.ravel())"],"id":"ZDLDtoo2Blh-"},{"cell_type":"code","execution_count":null,"metadata":{"id":"brpnndFoBlh-"},"outputs":[],"source":["# mejor resultado\n","print(\"Mejor: %f usando %s\" % (grid_result_logreg.best_score_, grid_result_logreg.best_params_))"],"id":"brpnndFoBlh-"},{"cell_type":"code","execution_count":null,"metadata":{"id":"kNEUl95JBlh_"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix"],"id":"kNEUl95JBlh_"},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfbxHkExBlh_"},"outputs":[],"source":["x_train_train_ml_dum_scaled, x_test_train_ml_dum_scaled, y_train_train_ml_2, y_test_train_ml_2 = train_test_split(x_train_ml_dum_scaled, \n","                                                                                                                  y_train_ml_2,\n","                                                                                                                  test_size=0.25,\n","                                                                                                                  random_state=0)\n","\n","\n","# armo la regresión logística con los mejores hiperparámetros encontrados\n","logreg = LogisticRegression(C= 10000000, penalty = 'l2', solver='newton-cg')\n","\n","logreg = logreg.fit(x_train_train_ml_dum_scaled.values, y_train_train_ml_2.values.ravel())"],"id":"tfbxHkExBlh_"},{"cell_type":"code","execution_count":null,"metadata":{"id":"erVNL1hBBliA"},"outputs":[],"source":["# Matriz de Confusión y cálculo de scores para logreg\n","y_pred_logreg = logreg.predict(x_test_train_ml_dum_scaled)\n","#matriz de confusion\n","#from slkearn.metrics import confusion_matrix \n","print('Matriz de confusion')\n","print('-------------------')\n","cm_logreg = confusion_matrix(y_test_train_ml_2,y_pred_logreg)\n","print(cm_logreg)\n","\n"],"id":"erVNL1hBBliA"},{"cell_type":"code","execution_count":null,"metadata":{"id":"OvGYhOjxBliB"},"outputs":[],"source":["y_pred_logreg_final = logreg.predict(df_test_dum_scaled)"],"id":"OvGYhOjxBliB"},{"cell_type":"markdown","metadata":{"id":"78W-ySEDBliB"},"source":["### Modelo 2: ***Bayes Ingenuo***"],"id":"78W-ySEDBliB"},{"cell_type":"code","execution_count":null,"metadata":{"id":"U8hpM4LnBliB"},"outputs":[],"source":["#librerias  \n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import classification_report\n","from sklearn import tree\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import plot_confusion_matrix\n"],"id":"U8hpM4LnBliB"},{"cell_type":"code","execution_count":null,"metadata":{"id":"66SReyU6BliC"},"outputs":[],"source":["cont=0\n","for i in x_dummie:\n","    print('================================================')\n","    if cont==0:\n","        print('Analisis numero 1')\n","        print('Modelo con dataset eliminando datos faltantes ya que hay baja correlacion con la variable a predecir y tienen poco peso sobre el total de los datos')\n","        print('================================================')\n","    elif cont==1: \n","        print('Analisis numero 2')\n","        print('Modelo con dataset reemplazando los valores faltantes por mediana en caso de series numericas y \"Desconocido\" para categoricas.')\n","        print('================================================')\n","    else:\n","        print('Analisis numero 3')\n","        print('Modelo con dataset reemplazando los valores faltantes numericos por el promedio y \"Desconocido\" para categoricas.')\n","        print('================================================')\n","   \n","    #divido en prueba y testeo\n","    #from sklearn.model_selection import train_test_split\n","    x_train, x_test, y_train, y_test = train_test_split(i, \n","                                          y_dummie[cont],\n","                                          test_size=0.25,\n","                                          random_state=0)\n","    #estandarizacion de escalas\n","    #from sklearn.preprocessing import StandardScaler\n","    sc = StandardScaler()\n","    x_train = sc.fit_transform(x_train)\n","    x_test = sc.fit_transform(x_test)\n","    #defino el objeto clasificador\n","    from sklearn.naive_bayes import GaussianNB\n","    classifier = GaussianNB()   \n","    #entreno el clasificador\n","    classifier.fit(x_train,y_train)   \n","    #predigo usando las x que guarde como test \n","    y_pred_bayes = classifier.predict(x_test)\n","    #matriz de confusion\n","    #from slkearn.metrics import confusion_matrix \n","    print('Matriz de confusion')\n","    print('-------------------')\n","    cm_bayes = confusion_matrix(y_test,y_pred_bayes)\n","    print(cm_bayes)\n","    #plot_confusion_matrix(classifier, x_train, y_train, cmap=plt.cm.Blues, values_format = '.0f')\n","    \n","    cont+=1"],"id":"66SReyU6BliC"},{"cell_type":"markdown","metadata":{"id":"5Oj7IKALBliD"},"source":["### Modelo 3: ***Árbol de Decisión***"],"id":"5Oj7IKALBliD"},{"cell_type":"code","execution_count":null,"metadata":{"id":"hV2BbYYLBliE"},"outputs":[],"source":["cont=0\n","arboles=[]\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import cross_validate\n","for i in x_dummie:\n","    print('================================================')\n","    if cont==0:\n","        print('Analisis numero 1')\n","        print('Modelo con dataset eliminando datos faltantes ya que hay baja correlacion con la variable a predecir y tienen poco peso sobre el total de los datos')\n","        print('================================================')\n","    elif cont==1:\n","        print('Analisis numero 2')\n","        print('Modelo con dataset reemplazando los valores faltantes por mediana en caso de series numericas y \"Desconocido\" para categoricas.')\n","        print('================================================')\n","    else:\n","        print('Analisis numero 3')\n","        print('Modelo con dataset reemplazando los valores faltantes numericos por el promedio y \"Desconocido\" para categoricas.')\n","        print('================================================')\n","    #divido en prueba y testeo\n","    #from sklearn.model_selection import train_test_split\n","    x_train, x_test, y_train, y_test = train_test_split(i, \n","                                          y_dummie[cont],\n","                                          test_size=0.25,\n","                                          random_state=0)\n","    #estandarizacion\n","    sc = StandardScaler()\n","    x_train = sc.fit_transform(x_train)\n","    x_test = sc.fit_transform(x_test)\n","    #from sklearn.tree import DecisionTreeClassifier\n","    clf = DecisionTreeClassifier(random_state=0)\n","    clf = clf.fit(x_train,y_train)\n","    '''print('Relevancia de cada campo:')\n","    feature_names = x_train.columns\n","    featureImportance = pd.DataFrame(clf.feature_importances_, index=feature_names)\n","    featureImportance.head(20)\n","    featureImportance.plot(kind='bar')'''\n","    print('---------------------------')\n","    print('Parametros del modelo')\n","    print('---------------------------')\n","    print(clf.get_params())\n","    y_pred_ad = clf.predict(x_test) \n","    #performance \n","    print('---------------------------')\n","    print(' Evaluacion de performance')\n","    print('---------------------------')\n","    #from sklearn.metrics import accuracy_score      \n","    acSc = accuracy_score(y_test, y_pred_ad)\n","    print('Accuracy = %s'%(acSc))  \n","    print('---------------------------')\n","    print('Matriz de confusion')\n","    #matriz de confusion\n","    #from slkearn.metrics import confusion_matrix \n","    cm_ad = confusion_matrix(y_test,y_pred_ad)\n","    print(cm_ad)\n","    print('---------------------------')\n","    #precision\n","    #from sklearn.metrics import precision_score\n","    prec = precision_score(y_test,y_pred_ad)\n","    print('Precision: %s'%(prec))\n","    print('---------------------------')\n","    #recall score\n","    #from sklearn.metrics import recall_score\n","    rec = recall_score(y_test,y_pred_ad)\n","    print('Recall Score: %s'%(rec))\n","    #reporte de performance total\n","    #from sklearn.metrics import classification_report\n","    #print('---------------------------')\n","    #print('Reporte de resultados:')\n","    #print('---------------------------')\n","    #classification_report(y_test, y_pred_ad)\n","    #grafico del arbol\n","    #'''print('Grafico')\n","    #print('---------------------------')\n","    #from sklearn import tree\n","    #fig = plt.figure(figsize=(25,20))\n","    #treePlot = clf.plot_tree(clf,\n","    #                          feature_names=feature_names,\n","    #                          class_names={0:'No suscripcion',1:'Suscripto'},\n","    #                          filled = True,\n","    #                          fontsize = 10)\n","    #treePlot.show()   ''' \n","    #optimizar modelo\n","    print('---------------------------')\n","    print('Optimizador de parametros')\n","    print('---------------------------')\n","    if cont==0:\n","        #from sklearn.model_selection import cross_val_score\n","        tree1 = DecisionTreeClassifier()\n","        tree_scores = cross_val_score(tree1, i, y_dummie[cont], cv=5)\n","        #Max depth optimo\n","        #from sklearn.model_selection import cross_validate\n","        tree_train_scores_mean = []\n","        tree_train_scores_std = []\n","        tree_test_scores_mean = []\n","        tree_test_scores_std = []\n","\n","        profundidades = np.arange(1,50,1)\n","\n","        for profundidad in profundidades:\n","            clf = DecisionTreeClassifier(max_depth=profundidad)\n","            tree_scores = cross_validate(clf, i, y_dummie[cont], cv=5, return_train_score=True, n_jobs = -1)\n","\n","            tree_train_scores_mean.append(tree_scores['train_score'].mean())\n","            tree_train_scores_std.append(tree_scores['train_score'].std())\n","\n","            tree_test_scores_mean.append(tree_scores['test_score'].mean())\n","            tree_test_scores_std.append(tree_scores['test_score'].std())\n","\n","        tree_train_scores_mean = np.array(tree_train_scores_mean)\n","        tree_train_scores_std = np.array(tree_train_scores_std)\n","        tree_test_scores_mean = np.array(tree_test_scores_mean)\n","        tree_test_scores_std = np.array(tree_test_scores_std)\n","\n","        plt.figure(figsize=(6,4))\n","        plt.fill_between(profundidades, tree_train_scores_mean - tree_train_scores_std,\n","                         tree_train_scores_mean + tree_train_scores_std, alpha=0.1,\n","                         color=\"r\")\n","        plt.fill_between(profundidades, tree_test_scores_mean - tree_test_scores_std,\n","                         tree_test_scores_mean + tree_test_scores_std, alpha=0.1, color=\"g\")\n","        plt.plot(profundidades, tree_train_scores_mean, 'o-', color=\"r\",\n","                 label=\"Training score\")\n","        plt.plot(profundidades, tree_test_scores_mean, 'o-', color=\"g\",\n","                 label=\"Test score\")\n","        plt.legend()\n","        plt.ylabel('Accuracy')\n","        plt.xlabel('Profundidad Arbol de Decision')\n","        plt.show()\n","        arboles.append(tree1)\n","        cont+=1\n","    elif cont==1:\n","        #from sklearn.model_selection import cross_val_score\n","        tree2 = DecisionTreeClassifier()\n","        tree_scores = cross_val_score(tree2, i, y_dummie[cont], cv=5)\n","        #Max depth optimo\n","        #from sklearn.model_selection import cross_validate\n","        tree_train_scores_mean = []\n","        tree_train_scores_std = []\n","        tree_test_scores_mean = []\n","        tree_test_scores_std = []\n","\n","        profundidades = np.arange(1,50,1)\n","\n","        for profundidad in profundidades:\n","            clf = DecisionTreeClassifier(max_depth=profundidad)\n","            tree_scores = cross_validate(clf, i, y_dummie[cont], cv=5, return_train_score=True, n_jobs = -1)\n","\n","            tree_train_scores_mean.append(tree_scores['train_score'].mean())\n","            tree_train_scores_std.append(tree_scores['train_score'].std())\n","\n","            tree_test_scores_mean.append(tree_scores['test_score'].mean())\n","            tree_test_scores_std.append(tree_scores['test_score'].std())\n","\n","        tree_train_scores_mean = np.array(tree_train_scores_mean)\n","        tree_train_scores_std = np.array(tree_train_scores_std)\n","        tree_test_scores_mean = np.array(tree_test_scores_mean)\n","        tree_test_scores_std = np.array(tree_test_scores_std)\n","\n","        plt.figure(figsize=(6,4))\n","        plt.fill_between(profundidades, tree_train_scores_mean - tree_train_scores_std,\n","                         tree_train_scores_mean + tree_train_scores_std, alpha=0.1,\n","                         color=\"r\")\n","        plt.fill_between(profundidades, tree_test_scores_mean - tree_test_scores_std,\n","                         tree_test_scores_mean + tree_test_scores_std, alpha=0.1, color=\"g\")\n","        plt.plot(profundidades, tree_train_scores_mean, 'o-', color=\"r\",\n","                 label=\"Training score\")\n","        plt.plot(profundidades, tree_test_scores_mean, 'o-', color=\"g\",\n","                 label=\"Test score\")\n","        plt.legend()\n","        plt.ylabel('Accuracy')\n","        plt.xlabel('Profundidad Arbol de Decision')\n","        plt.show()\n","        arboles.append(tree2)\n","        cont+=1\n","    else:\n","        #from sklearn.model_selection import cross_val_score\n","        tree3 = DecisionTreeClassifier()\n","        tree_scores = cross_val_score(tree3, i, y_dummie[cont], cv=5)\n","        #Max depth optimo\n","        #from sklearn.model_selection import cross_validate\n","        tree_train_scores_mean = []\n","        tree_train_scores_std = []\n","        tree_test_scores_mean = []\n","        tree_test_scores_std = []\n","\n","        profundidades = np.arange(1,50,1)\n","\n","        for profundidad in profundidades:\n","            clf = DecisionTreeClassifier(max_depth=profundidad)\n","            tree_scores = cross_validate(clf, i, y_dummie[cont], cv=5, return_train_score=True, n_jobs = -1)\n","\n","            tree_train_scores_mean.append(tree_scores['train_score'].mean())\n","            tree_train_scores_std.append(tree_scores['train_score'].std())\n","\n","            tree_test_scores_mean.append(tree_scores['test_score'].mean())\n","            tree_test_scores_std.append(tree_scores['test_score'].std())\n","\n","        tree_train_scores_mean = np.array(tree_train_scores_mean)\n","        tree_train_scores_std = np.array(tree_train_scores_std)\n","        tree_test_scores_mean = np.array(tree_test_scores_mean)\n","        tree_test_scores_std = np.array(tree_test_scores_std)\n","\n","        plt.figure(figsize=(6,4))\n","        plt.fill_between(profundidades, tree_train_scores_mean - tree_train_scores_std,\n","                         tree_train_scores_mean + tree_train_scores_std, alpha=0.1,\n","                         color=\"r\")\n","        plt.fill_between(profundidades, tree_test_scores_mean - tree_test_scores_std,\n","                         tree_test_scores_mean + tree_test_scores_std, alpha=0.1, color=\"g\")\n","        plt.plot(profundidades, tree_train_scores_mean, 'o-', color=\"r\",\n","                 label=\"Training score\")\n","        plt.plot(profundidades, tree_test_scores_mean, 'o-', color=\"g\",\n","                 label=\"Test score\")\n","        plt.legend()\n","        plt.ylabel('Accuracy')\n","        plt.xlabel('Profundidad Arbol de Decision')\n","        plt.show()\n","        arboles.append(tree3)\n","        cont+=1"],"id":"hV2BbYYLBliE"},{"cell_type":"markdown","metadata":{"id":"PBYrDKMuBliG"},"source":["Vemos que la maxima profunidad para la cual no se ve un incremento de accuracy es aproximadamente en 25. A conitnuacion se busca optimizar los parametros del modelo; para mejorar el tiempo de procesamiento dejamos fijo el max depth en 25."],"id":"PBYrDKMuBliG"},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2yQk6yzBliH"},"outputs":[],"source":["for i in [tree1,tree2,tree3]:\n","    print(i.get_params())"],"id":"G2yQk6yzBliH"},{"cell_type":"code","execution_count":null,"metadata":{"id":"N-6kedFEBliI"},"outputs":[],"source":["# ESTRATEGIA : Grid Search\n","#dataset 1\n","\n","param_grid = {'max_depth':[25],\n","              'criterion': ['gini', 'entropy'], \n","              'min_samples_split':[4,5,6,7,8],\n","              'min_samples_leaf':[4,5,6,7,8]}\n","model1 = GridSearchCV(arboles[0], param_grid=param_grid, cv=5)\n","# Entrenamos: Arbol de decision con la grilla definida arriba y CV con tamaño de Fold=5\n","model1.fit(x_train, y_train)\n","print('---------------------------')\n","print(\"Mejores parametros: \"+str(model1.best_params_))\n","print('---------------------------')\n","print(\"Mejor Score: \"+str(round(model1.best_score_,4))+'\\n')\n","scores1 = pd.DataFrame(model1.cv_results_)\n","scores1\n","prediction1 = model1.predict(x_test)\n","print('---------------------------')\n","print(\"Matriz de confusión:\")\n","print('---------------------------')\n","cm1_ad= confusion_matrix(y_test,prediction1)\n","print(cm1_ad)\n","print('---------------------------')\n","print('Accuracy Score:', accuracy_score(y_test, prediction1))\n","print('---------------------------')\n","print('Precision Score:', precision_score(y_test, prediction1))\n","print('---------------------------')\n","print('Recall Score:', recall_score(y_test, prediction1))\n","print('---------------------------')\n","# Reporte de Clasificacion\n","#print(\"Reporte de Clasificación:\")\n","#print('---------------------------')\n","#print(classification_report(y_test, prediction1))"],"id":"N-6kedFEBliI"},{"cell_type":"code","execution_count":null,"metadata":{"id":"BqPN7UCiBliJ"},"outputs":[],"source":["# ESTRATEGIA : Grid Search\n","#dataset 2\n","\n","param_grid2 = {'max_depth':[25],\n","              'criterion': ['gini', 'entropy'], \n","              'min_samples_split':[4,5,6,7,8],\n","              'min_samples_leaf':[4,5,6,7,8,]}\n","model2 = GridSearchCV(arboles[1], param_grid=param_grid2, cv=5)\n","# Entrenamos: Arbol de decision con la grilla definida arriba y CV con tamaño de Fold=5\n","model2.fit(x_train, y_train)\n","print('---------------------------')\n","print(\"Mejores parametros: \"+str(model2.best_params_))\n","print(\"Best Score: \"+str(round(model2.best_score_,4))+'\\n')\n","scores2 = pd.DataFrame(model2.cv_results_)\n","scores2\n","prediction2 = model2.predict(x_test)\n","print('---------------------------')\n","print(\"Matriz de confusión:\")\n","print('---------------------------')\n","cm2_ad= confusion_matrix(y_test,prediction2)\n","print(cm2_ad)\n","print('---------------------------')\n","print('Accuracy Score:', accuracy_score(y_test, prediction2))\n","print('---------------------------')\n","print('Precision Score:', precision_score(y_test, prediction2))\n","print('---------------------------')\n","print('Recall Score:', recall_score(y_test, prediction2))\n","print('---------------------------')\n","# Reporte de Clasificacion\n","#print(\"Reporte de Clasificación:\")\n","#print('---------------------------')\n","#print(classification_report(y_test, prediction2))"],"id":"BqPN7UCiBliJ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"aW8w76PtBliK"},"outputs":[],"source":["# ESTRATEGIA : Grid Search\n","#dataset 3\n","\n","param_grid = {'max_depth':[25],\n","              'criterion': ['gini', 'entropy'], \n","              'min_samples_split':[4,5,6,7,8],\n","              'min_samples_leaf':[4,5,6,7,8,]}\n","model3 = GridSearchCV(arboles[2], param_grid=param_grid, cv=5)\n","# Entrenamos: Arbol de decision con la grilla definida arriba y CV con tamaño de Fold=5\n","model3.fit(x_train, y_train)\n","print('---------------------------')\n","print(\"Mejores parametros: \"+str(model3.best_params_))\n","print('---------------------------')\n","print(\"Mejor Score: \"+str(model3.best_score_)+'\\n')\n","print('---------------------------')\n","scores3 = pd.DataFrame(model3.cv_results_)\n","scores3\n","prediction3 = model3.predict(x_test)\n","print(\"Matriz de confusión:\")\n","cm3_ad= confusion_matrix(y_test,prediction3)\n","print(cm2_ad)\n","print('---------------------------')\n","print('Exactitud:', accuracy_score(y_test, prediction3))\n","print('---------------------------')\n","print('Precision Score:', precision_score(y_test, prediction3))\n","print('---------------------------')\n","print('Recall Score:', recall_score(y_test, prediction3))\n","print('---------------------------')\n","#Reporte de Clasificacion\n","#print(\"Reporte de Clasificación:\")\n","#print('---------------------------')\n","#print(classification_report(y_test, prediction3))"],"id":"aW8w76PtBliK"},{"cell_type":"markdown","metadata":{"id":"zjfLb0CXBliM"},"source":["### Modelo 4: ***XGBoost***"],"id":"zjfLb0CXBliM"},{"cell_type":"code","execution_count":null,"metadata":{"id":"oFl5R_ayBliM"},"outputs":[],"source":["#librerias \n","#!pip install xgboost\n","import xgboost"],"id":"oFl5R_ayBliM"},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFfngp8zBliN"},"outputs":[],"source":["cont=0\n","for i in [df_train_1,df_train_2,df_train_3]:\n","    print('================================================')\n","    cont+=1\n","    if cont==1:\n","        print('Analisis numero 1')\n","        print('Modelo con dataset eliminando datos faltantes ya que hay baja correlacion con la variable a predecir y tienen poco peso sobre el total de los datos')\n","        print('================================================')\n","    elif cont==2:\n","        print('Analisis numero 2')\n","        print('Modelo con dataset reemplazando los valores faltantes por mediana en caso de series numericas y \"Desconocido\" para categoricas.')\n","        print('================================================')\n","    else:\n","        print('Analisis numero 3')\n","        print('Modelo con dataset reemplazando los valores faltantes numericos por el promedio y \"Desconocido\" para categoricas.')\n","        print('================================================')\n","    #armo lista de variables categoricas para transformar valor a frecuencias\n","    categoricas = []\n","    for j in i.columns:\n","        if i[j].dtype == 'O':\n","            categoricas.append(j)\n","    for cat in categoricas:\n","        i[cat]=i.groupby(cat)[cat].transform('count') #transformo valor a frecuencia\n","    i.head()\n","    # variables explicativas\n","    x_train_ml = i.loc[:, i.columns != 'Subscription'] \n","    # variables de respuesta\n","    y_train_ml = i[['Subscription']] \n","    #separo la muestra en train y test\n","    x_train, x_test, y_train, y_test = train_test_split(x_train_ml, \n","                                          y_train_ml,\n","                                          test_size=0.25,\n","                                          random_state=0)\n","    #separo prueba y validacion\n","    x_valid, x_test, y_valid, y_test = train_test_split(x_test, \n","                                          y_test,\n","                                          test_size=0.50,\n","                                          random_state=0)\n","    #import xgboost\n","    xgb1 = xgboost.XGBClassifier(eta=0.5,max_depth=10)\n","    xgb2 = xgboost.XGBClassifier(eta=0.6,max_depth=20)\n","    xgb3 = xgboost.XGBClassifier(eta=0.7,max_depth=30)\n","    xgb4 = xgboost.XGBClassifier(eta=0.8,max_depth=40)\n","    #optimizar parametros\n","    '''params = {'nthreads':[1], # numero de hilos\n","              'objective':['binary:logistic'],# tipo de clasificacion entre 0 y 1\n","              'learning_rate':[0.05,0.1], #tasa de aprendizaje, se entrena con ambos\n","              'n_estimators':[100,200]} # numero de arboles, boosting basado en arboles, corre para cada valor.\n","    #importo para hacer validacion cruzada\n","    #from sklearn.model_selection import GridSearchCV\n","    #parametros de entrenamiento para que no haya sobreentrenamiento\n","    #dar valor de corte\n","    fit_params = {'early_stoping_round':10, # si en 10 rondas no mejora, detiene entrenamiento\n","                  'eval_metric':'logloss', #funcion de perdida logaritmica   \n","                  'eval_set':[(x_test, y_test)]}\n","    clf_xgb = GridSearchCV(xgb, params, \n","                       fit_params=fit_params,\n","                       cv=3, #3 pliegues por todas las combinaciones seteadas en params\n","                       scoring='accuracy')'''\n","    xgb1.fit(x_train,y_train)\n","    xgb2.fit(x_train,y_train)\n","    xgb3.fit(x_train,y_train)\n","    xgb4.fit(x_train,y_train)\n","    #best_xgb = xgb.best_estimator_\n","    #print('Mejor estimador: %s'%(best_xgb))\n","    #print('Mejor score: %s'%(xgb.best_score_))\n","    #predicciones con valid\n","    y_pred_xgb1 = xgb1.predict(x_valid)\n","    y_pred_xgb2 = xgb2.predict(x_valid)\n","    y_pred_xgb3 = xgb3.predict(x_valid)\n","    y_pred_xgb4 = xgb4.predict(x_valid)\n","    #armo df con reales y predichos\n","    #comparar = pd.DataFrame({'real':y_valid,'predicho':y_pred_xgb})\n","    print('---------------------------')\n","    print(\"Matriz de confusión: eta=0.5,max_depth=10\")\n","    print('---------------------------')\n","    cm_xgb1= confusion_matrix(y_valid,y_pred_xgb1)\n","    print(cm_xgb1)\n","    #accuracy\n","    print('Exactitud: ',accuracy_score(y_valid,y_pred_xgb1))\n","    #se puede mejorar aumentando la cantidad de arboles pero aumenta el tiempo de procesamiento\n","    #tambien puede haber sobre entrenamiento con tantos arboles\n","    print('---------------------------')\n","    print(\"Matriz de confusión: eta=0.6,max_depth=20\")\n","    print('---------------------------')\n","    cm_xgb2= confusion_matrix(y_valid,y_pred_xgb2)\n","    print(cm_xgb2)\n","    #accuracy\n","    print('Exactitud: ',accuracy_score(y_valid,y_pred_xgb2))\n","    print('---------------------------')\n","    print(\"Matriz de confusión: eta=0.7,max_depth=30\")\n","    print('---------------------------')\n","    cm_xgb3= confusion_matrix(y_valid,y_pred_xgb3)\n","    print(cm_xgb3)\n","    #accuracy\n","    print('Exactitud: ',accuracy_score(y_valid,y_pred_xgb3))\n","    print('---------------------------')\n","    print(\"Matriz de confusión: eta=0.8,max_depth=40\")\n","    print('---------------------------')\n","    cm_xgb4= confusion_matrix(y_valid,y_pred_xgb4)\n","    print(cm_xgb4)\n","    #accuracy\n","    print('Exactitud: ',accuracy_score(y_valid,y_pred_xgb4))\n","    \n","    "],"id":"MFfngp8zBliN"},{"cell_type":"markdown","metadata":{"id":"G9JxLB3NBliO"},"source":["A partir de estos resultados observamos que el **XGBoost con los hiperparámetros de eta=0.6 y max_depth = 20**, es el mejor modelo para predecir en este caso:"],"id":"G9JxLB3NBliO"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChpWsoaOBliP"},"outputs":[],"source":["del df_test['Total_loan'] # lo sacamos porque daba la mismo info que Deudor"],"id":"ChpWsoaOBliP"},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"Y1Zni_nxBliP"},"outputs":[],"source":["# predicciones finales para el XGBoost\n","\n","cont=0\n","for i in [df_train_2]:\n","    print('================================================')\n","    cont+=1\n","    if cont==1:\n","        print('Analisis numero 1')\n","        print('Modelo con dataset eliminando datos faltantes ya que hay baja correlacion con la variable a predecir y tienen poco peso sobre el total de los datos')\n","        print('================================================')\n","    elif cont==2:\n","        print('Analisis numero 2')\n","        print('Modelo con dataset reemplazando los valores faltantes por mediana en caso de series numericas y \"Desconocido\" para categoricas.')\n","        print('================================================')\n","    else:\n","        print('Analisis numero 3')\n","        print('Modelo con dataset reemplazando los valores faltantes numericos por el promedio y \"Desconocido\" para categoricas.')\n","        print('================================================')\n","    #armo lista de variables categoricas para transformar valor a frecuencias\n","    categoricas = []\n","    for j in i.columns:\n","        if i[j].dtype == 'O':\n","            categoricas.append(j)\n","    for cat in categoricas:\n","        i[cat]=i.groupby(cat)[cat].transform('count') #transformo valor a frecuencia\n","    i.head()\n","    # variables explicativas\n","    x_train_ml = i.loc[:, i.columns != 'Subscription'] \n","    # variables de respuesta\n","    y_train_ml = i[['Subscription']] \n","    #separo la muestra en train y test\n","    x_train, x_test, y_train, y_test = train_test_split(x_train_ml, \n","                                          y_train_ml,\n","                                          test_size=0.25,\n","                                          random_state=0)\n","    #separo prueba y validacion\n","    x_valid, x_test, y_valid, y_test = train_test_split(x_test, \n","                                          y_test,\n","                                          test_size=0.50,\n","                                          random_state=0)\n","    #import xgboost\n","    xgb1 = xgboost.XGBClassifier(eta=0.5,max_depth=10)\n","    xgb2 = xgboost.XGBClassifier(eta=0.6,max_depth=20)\n","    xgb3 = xgboost.XGBClassifier(eta=0.7,max_depth=30)\n","    xgb4 = xgboost.XGBClassifier(eta=0.8,max_depth=40)\n","    #optimizar parametros\n","    '''params = {'nthreads':[1], # numero de hilos\n","              'objective':['binary:logistic'],# tipo de clasificacion entre 0 y 1\n","              'learning_rate':[0.05,0.1], #tasa de aprendizaje, se entrena con ambos\n","              'n_estimators':[100,200]} # numero de arboles, boosting basado en arboles, corre para cada valor.\n","    #importo para hacer validacion cruzada\n","    #from sklearn.model_selection import GridSearchCV\n","    #parametros de entrenamiento para que no haya sobreentrenamiento\n","    #dar valor de corte\n","    fit_params = {'early_stoping_round':10, # si en 10 rondas no mejora, detiene entrenamiento\n","                  'eval_metric':'logloss', #funcion de perdida logaritmica   \n","                  'eval_set':[(x_test, y_test)]}\n","    clf_xgb = GridSearchCV(xgb, params, \n","                       fit_params=fit_params,\n","                       cv=3, #3 pliegues por todas las combinaciones seteadas en params\n","                       scoring='accuracy')'''\n","    xgb1.fit(x_train,y_train)\n","    xgb2.fit(x_train,y_train)\n","    xgb3.fit(x_train,y_train)\n","    xgb4.fit(x_train,y_train)\n","    #best_xgb = xgb.best_estimator_\n","    #print('Mejor estimador: %s'%(best_xgb))\n","    #print('Mejor score: %s'%(xgb.best_score_))\n","    #predicciones con valid\n","    y_pred_xgb1 = xgb1.predict(x_valid)\n","    y_pred_xgb2 = xgb2.predict(x_valid)\n","    y_pred_xgb3 = xgb3.predict(x_valid)\n","    y_pred_xgb4 = xgb4.predict(x_valid)\n","    #armo df con reales y predichos\n","    #comparar = pd.DataFrame({'real':y_valid,'predicho':y_pred_xgb})\n","    print('---------------------------')\n","    print(\"Matriz de confusión: eta=0.5,max_depth=10\")\n","    print('---------------------------')\n","    cm_xgb1= confusion_matrix(y_valid,y_pred_xgb1)\n","    print(cm_xgb1)\n","    #accuracy\n","    print('Exactitud: ',accuracy_score(y_valid,y_pred_xgb1))\n","    #se puede mejorar aumentando la cantidad de arboles pero aumenta el tiempo de procesamiento\n","    #tambien puede haber sobre entrenamiento con tantos arboles\n","    print('---------------------------')\n","    print(\"Matriz de confusión: eta=0.6,max_depth=20\")\n","    print('---------------------------')\n","    cm_xgb2= confusion_matrix(y_valid,y_pred_xgb2)\n","    print(cm_xgb2)\n","    #accuracy\n","    print('Exactitud: ',accuracy_score(y_valid,y_pred_xgb2))\n","    print('---------------------------')\n","    print(\"Matriz de confusión: eta=0.7,max_depth=30\")\n","    print('---------------------------')\n","    cm_xgb3= confusion_matrix(y_valid,y_pred_xgb3)\n","    print(cm_xgb3)\n","    #accuracy\n","    print('Exactitud: ',accuracy_score(y_valid,y_pred_xgb3))\n","    print('---------------------------')\n","    print(\"Matriz de confusión: eta=0.8,max_depth=40\")\n","    print('---------------------------')\n","    cm_xgb4= confusion_matrix(y_valid,y_pred_xgb4)\n","    print(cm_xgb4)\n","    #accuracy\n","    print('Exactitud: ',accuracy_score(y_valid,y_pred_xgb4))\n","    \n","    categoricas_2 = []\n","    for j in df_test.columns:\n","        if df_test[j].dtype == 'O':\n","            categoricas_2.append(j)\n","    for cat in categoricas_2:\n","        df_test[cat]=df_test.groupby(cat)[cat].transform('count') #transformo valor a frecuencia\n","    df_test.head()\n","    \n","    y_pred_xgb2_final = xgb2.predict(df_test)\n"],"id":"Y1Zni_nxBliP"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dTA9wFbGBliQ"},"outputs":[],"source":["df_test['y_Subscrip_predichos'] = y_pred_xgb2_final"],"id":"dTA9wFbGBliQ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwE7ixchBliR"},"outputs":[],"source":["df_test.head()"],"id":"FwE7ixchBliR"},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTZrlGAzBliS"},"outputs":[],"source":["#exportamos los valores predichos finales\n","\n","df_test.to_csv('Predicciones finales Subscriptions.csv',index=False)"],"id":"CTZrlGAzBliS"},{"cell_type":"code","execution_count":null,"metadata":{"id":"lydnVEglBliU"},"outputs":[],"source":[],"id":"lydnVEglBliU"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}